{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T04:26:23.745753Z",
     "iopub.status.busy": "2020-09-23T04:26:23.745066Z",
     "iopub.status.idle": "2020-09-23T04:26:32.715464Z",
     "shell.execute_reply": "2020-09-23T04:26:32.714716Z"
    },
    "id": "WZKbyU2-AiY-"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "@ author: Taehyeong Kim, Fusion Data Analytics and Artificial Intelligence Lab\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, initializers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "SEED=1011\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_seeds()\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from watermark import watermarking\n",
    "from skimage import color\n",
    "\n",
    "dataset = 'cifar10'\n",
    "alpha = 0\n",
    "\n",
    "if dataset=='cifar10':\n",
    "    (train_images, train_labels), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
    "elif dataset=='cifar100':\n",
    "    (train_images, train_labels), (_, _) = tf.keras.datasets.cifar100.load_data(label_mode='coarse')\n",
    "\n",
    "if alpha != 0:\n",
    "    train_images=watermarking(train_images, alpha)\n",
    "    train_images=(train_images - 0.5) / 0.5\n",
    "else:\n",
    "    train_images=(train_images - 127.5) / 127.5\n",
    "\n",
    "train_images=train_images.astype(\"float32\")\n",
    "print(train_images.shape, train_labels.max())\n",
    "\n",
    "BUFFER_SIZE = train_images.shape[0]\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T04:26:35.914564Z",
     "iopub.status.busy": "2020-09-23T04:26:35.913743Z",
     "iopub.status.idle": "2020-09-23T04:26:35.916165Z",
     "shell.execute_reply": "2020-09-23T04:26:35.915647Z"
    },
    "id": "6bpTcDqoLWjY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_classes=train_labels.max()+1\n",
    "latent_dim=100\n",
    "\n",
    "def make_generator_model():\n",
    "\n",
    "    in_label = tf.keras.Input(shape=(1,))\n",
    "    li = layers.Embedding(n_classes, 50)(in_label)\n",
    "    n_nodes = 4 * 4\n",
    "    li = layers.Dense(n_nodes,\n",
    "                      kernel_initializer=initializers.RandomNormal(stddev=0.02))(li)\n",
    "    li = layers.Reshape((4, 4, 1))(li)\n",
    "\n",
    "    in_lat = tf.keras.Input(shape=(latent_dim,))\n",
    "\n",
    "    gen = layers.Dense(4*4*512,\n",
    "                       use_bias=False, kernel_initializer=initializers.RandomNormal(stddev=0.02))(in_lat)\n",
    "    gen = layers.BatchNormalization()(gen)\n",
    "    gen = layers.LeakyReLU(0.2)(gen)\n",
    "\n",
    "    gen = layers.Reshape((4, 4, 512))(gen)\n",
    "    merge = layers.Concatenate()([gen, li])\n",
    "\n",
    "    gen = layers.Conv2DTranspose(256, 5, strides=(2, 2), padding='same',\n",
    "                                 use_bias=False, kernel_initializer=initializers.RandomNormal(stddev=0.02))(merge)\n",
    "    gen = layers.BatchNormalization()(gen)\n",
    "    gen = layers.LeakyReLU(0.2)(gen)\n",
    "\n",
    "    gen = layers.Conv2DTranspose(128, 5, strides=(2, 2), padding='same',\n",
    "                                 use_bias=False, kernel_initializer=initializers.RandomNormal(stddev=0.02))(gen)\n",
    "    gen = layers.BatchNormalization()(gen)\n",
    "    gen = layers.LeakyReLU(0.2)(gen)\n",
    "\n",
    "    out_layer = layers.Conv2DTranspose(3, 5, strides=(2, 2), padding='same', activation='tanh',\n",
    "                                       use_bias=False, kernel_initializer=initializers.RandomNormal(stddev=0.02))(gen)\n",
    "\n",
    "    model = tf.keras.Model([in_lat, in_label], out_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "\n",
    "    in_label = tf.keras.Input(shape=(1,))\n",
    "    li = layers.Embedding(n_classes, 50)(in_label)\n",
    "    n_nodes = 32 * 32\n",
    "    li = layers.Dense(n_nodes,\n",
    "                      kernel_initializer=initializers.RandomNormal(stddev=0.02))(li)\n",
    "    li = layers.Reshape((32, 32, 1))(li)\n",
    "\n",
    "    in_image = tf.keras.Input(shape=(32, 32, 3))\n",
    "    merge = layers.Concatenate()([in_image, li])\n",
    "\n",
    "    fe = layers.Conv2D(64, 5, strides=(2, 2), padding='same',\n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.02))(merge)\n",
    "    fe = layers.LeakyReLU(0.2)(fe)\n",
    "    fe = layers.Dropout(0.3)(fe)\n",
    "\n",
    "    fe = layers.Conv2D(128, 5, strides=(2, 2), padding='same',\n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.02))(fe)\n",
    "    fe = layers.LayerNormalization()(fe)\n",
    "    fe = layers.LeakyReLU(0.2)(fe)\n",
    "    fe = layers.Dropout(0.3)(fe)\n",
    "\n",
    "    fe = layers.Conv2D(256, 5, strides=(2, 2), padding='same',\n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.02))(fe)\n",
    "    fe = layers.LayerNormalization()(fe)\n",
    "    fe = layers.LeakyReLU(0.2)(fe)\n",
    "    fe = layers.Dropout(0.3)(fe)\n",
    "\n",
    "    fe = layers.Flatten()(fe)\n",
    "    out_layer = layers.Dense(1, activation='linear',\n",
    "                             kernel_initializer=initializers.RandomNormal(stddev=0.02))(fe)\n",
    "\n",
    "    model = tf.keras.Model([in_image, in_label], out_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "generator = make_generator_model()\n",
    "generator.summary()\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(real, fake, epsilon, label):\n",
    "\n",
    "    mixed_images = fake + epsilon * (real - fake)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(mixed_images) \n",
    "        mixed_scores = discriminator([mixed_images, label])\n",
    "\n",
    "    gradient = tape.gradient(mixed_scores, mixed_images)[0]\n",
    "    gradient_norm = tf.norm(gradient)\n",
    "    penalty = tf.math.reduce_mean((gradient_norm - 1)**2)\n",
    "\n",
    "    return penalty\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    gen_loss = -1. * tf.math.reduce_mean(fake_output)\n",
    "    return gen_loss\n",
    "\n",
    "def discriminator_loss(real_output, fake_output, gradient_penalty):\n",
    "    c_lambda = 10\n",
    "    loss = tf.math.reduce_mean(fake_output) - tf.math.reduce_mean(real_output) + c_lambda * gradient_penalty\n",
    "    return loss\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0, beta_2=0.9)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0, beta_2=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T04:26:36.448587Z",
     "iopub.status.busy": "2020-09-23T04:26:36.447879Z",
     "iopub.status.idle": "2020-09-23T04:26:36.450031Z",
     "shell.execute_reply": "2020-09-23T04:26:36.450461Z"
    },
    "id": "NS2GWywBbAWo"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './load_model/generator'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "EPOCHS = 300\n",
    "\n",
    "num_examples_to_generate = n_classes\n",
    "noise_dim = latent_dim\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "label = np.arange(0, n_classes, 1)\n",
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T04:26:36.457358Z",
     "iopub.status.busy": "2020-09-23T04:26:36.456712Z",
     "iopub.status.idle": "2020-09-23T04:26:36.458607Z",
     "shell.execute_reply": "2020-09-23T04:26:36.459040Z"
    },
    "id": "3t5ibNo05jCB"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([images[0].shape[0], noise_dim])\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        with tf.GradientTape() as disc_tape:\n",
    "\n",
    "            generated_images = generator([noise, images[1]], training=True)\n",
    "            \n",
    "            real_output = discriminator([images[0], images[1]], training=True)\n",
    "            fake_output = discriminator([generated_images, images[1]], training=True)\n",
    "            \n",
    "            epsilon = tf.random.normal([images[0].shape[0], 1, 1, 1], 0.0, 1.0)\n",
    "            gp = gradient_penalty(images[0], generated_images, epsilon, images[1])\n",
    "        \n",
    "            disc_loss = discriminator_loss(real_output, fake_output, gp)\n",
    "\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "        \n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        \n",
    "        generated_images = generator([noise, images[1]], training=True)\n",
    "        \n",
    "        fake_output = discriminator([generated_images, images[1]], training=True)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T04:26:36.473053Z",
     "iopub.status.busy": "2020-09-23T04:26:36.472379Z",
     "iopub.status.idle": "2020-09-23T04:26:36.474229Z",
     "shell.execute_reply": "2020-09-23T04:26:36.474636Z"
    },
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input, test_label):\n",
    "\n",
    "    if len(test_label)==10:\n",
    "        width=5\n",
    "        height=2\n",
    "    elif len(test_label)==20:\n",
    "        width=5\n",
    "        height=4\n",
    "\n",
    "    predictions = model([test_input, test_label], training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(height, width, i+1)\n",
    "\n",
    "        R=predictions[i, :, :, 0] * 127.5 + 127.5\n",
    "        G=predictions[i, :, :, 1] * 127.5 + 127.5\n",
    "        B=predictions[i, :, :, 2] * 127.5 + 127.5\n",
    "\n",
    "        sample=np.stack([R,G,B], axis=2).round().astype(\"int\")\n",
    "\n",
    "        plt.imshow(sample)\n",
    "        plt.axis('off')\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        plt.savefig('./figure/forgery/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T04:26:36.465062Z",
     "iopub.status.busy": "2020-09-23T04:26:36.464426Z",
     "iopub.status.idle": "2020-09-23T04:26:36.466310Z",
     "shell.execute_reply": "2020-09-23T04:26:36.466708Z"
    },
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,\n",
    "                                 epoch + 1,\n",
    "                                 seed, label)\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epochs,\n",
    "                             seed, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T04:26:36.479786Z",
     "iopub.status.busy": "2020-09-23T04:26:36.479148Z",
     "iopub.status.idle": "2020-09-23T06:52:40.800934Z",
     "shell.execute_reply": "2020-09-23T06:52:40.801458Z"
    },
    "id": "Ly3UN0SLLY2l"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples, classes):\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    labels = np.full(n_samples, classes)\n",
    "    return [z_input, labels]\n",
    " \n",
    "def save_plot(examples, n):\n",
    "    for i in range(n):\n",
    "        plt.axis('off')\n",
    "        plt.imshow(examples[i, :, :, :])\n",
    "        plt.imsave('./data/forgery/fig{:d}.png'.format(i+1), examples[i])\n",
    "    plt.show()\n",
    "\n",
    "latent_points, labels = generate_latent_points(latent_dim=latent_dim,\n",
    "                                               n_samples=1000,\n",
    "                                               classes=0)\n",
    "X = generator.predict([latent_points, labels])\n",
    "X = (X + 1) / 2.0\n",
    "save_plot(X, 500)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
